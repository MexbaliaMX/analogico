import numpy as np
from hypothesis import given, settings, strategies as st
from src.hp_inv import hp_inv
from src.rram_model import create_rram_matrix

# Strategy for matrix size
size_strategy = st.integers(min_value=4, max_value=16)

# Strategy for float values
float_strategy = st.floats(min_value=-10.0, max_value=10.0, allow_nan=False, allow_infinity=False)

@st.composite
def well_conditioned_matrix_and_vector(draw, size_st=size_strategy):
    """
    Generates a well-conditioned matrix G and a vector b.
    Ensures G is diagonally dominant to avoid singularity.
    """
    n = draw(size_st)
    
    # Generate random matrix elements
    elements = draw(st.lists(
        st.lists(float_strategy, min_size=n, max_size=n),
        min_size=n, max_size=n
    ))
    G = np.array(elements)
    
    # Make strictly diagonally dominant to ensure invertibility
    # Sum of absolute values of off-diagonal elements
    off_diag_sum = np.sum(np.abs(G), axis=1) - np.abs(np.diag(G))
    
    # Add to diagonal to ensure |G_ii| > sum(|G_ij|)
    # Use a stronger factor (4.0) to survive quantization and ensure easy convergence
    G = G + np.eye(n) * (off_diag_sum + np.abs(np.diag(G)) + 4.0)
    
    # Ensure diagonal elements are not too small relative to noise
    for i in range(n):
        if abs(G[i, i]) < 1.0:
             G[i, i] += np.sign(G[i, i]) * 1.0 if G[i, i] != 0 else 1.0
    
    # Generate vector b
    b_elements = draw(st.lists(float_strategy, min_size=n, max_size=n))
    b = np.array(b_elements)
    
    return G, b

class TestHPInvProperties:
    
    @settings(max_examples=50, deadline=None)
    @given(well_conditioned_matrix_and_vector())
    def test_hp_inv_convergence(self, inputs):
        """
        Property: For a well-conditioned matrix, HP-INV should converge 
        to a solution with a small residual.
        """
        G, b = inputs
        
        # Run HP-INV with no noise to test algorithmic properties
        x, iters, info = hp_inv(G, b, bits=4, max_iter=20, tol=1e-5, lp_noise_std=0.0)
        
        # Check convergence
        # We check normalized residual
        b_norm = np.linalg.norm(b)
        if b_norm > 1e-9:
            norm_residual = info['final_residual'] / b_norm
        else:
            norm_residual = info['final_residual']
            
        # Without noise, should converge well
        assert norm_residual < 0.05 or info['converged'], \
            f"Failed to converge. Normalized Residual: {norm_residual}"
            
    @settings(max_examples=20, deadline=None)
    @given(well_conditioned_matrix_and_vector())
    def test_linearity_scaling(self, inputs):
        """
        Property: hp_inv(G, k*b) should be approximately k * hp_inv(G, b)
        """
        G, b = inputs
        k = 2.5
        
        # Run with no noise
        x1, _, _ = hp_inv(G, b, bits=4, max_iter=20, tol=1e-6, lp_noise_std=0.0)
        x2, _, _ = hp_inv(G, k * b, bits=4, max_iter=20, tol=1e-6, lp_noise_std=0.0)
        
        # Check scaling
        # x2 should be close to k * x1
        # We use a relative tolerance
        error = np.linalg.norm(x2 - k * x1)
        ref_norm = np.linalg.norm(k * x1)
        
        if ref_norm > 1e-6:
            rel_error = error / ref_norm
            # Strict tolerance without noise
            assert rel_error < 0.01, f"Linearity violation: rel_error {rel_error}"
            
    @settings(max_examples=20, deadline=None)
    @given(st.integers(min_value=4, max_value=16))
    def test_rram_matrix_generation_properties(self, n):
        """
        Property: RRAM matrices generated by the model should be non-negative
        and have the correct shape.
        """
        G = create_rram_matrix(n, variability=0.1)
        
        assert G.shape == (n, n)
        assert np.all(G >= 0), "RRAM conductance matrix must be non-negative"
